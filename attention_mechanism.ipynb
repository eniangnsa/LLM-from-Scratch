{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the attention Mechanism\n",
    "\n",
    "We will implement 4 attention mechanism\n",
    "- Simplified self-attention\n",
    "- Self-attention\n",
    "- Causal attention\n",
    "- Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  why do we need  the attention mechanism?\n",
    "\n",
    "Before diving into the details of how the attention mechanism works, it is better to first understand the S-O-T-A models then, which was the RNN encoder-decoder network. This network had its own advantages and was very useful for language translation. It works based on the  principle of using the previous state value and the current input, to compute the next state. This works well for short sentence and texts. But in cases of long sentence, it seems to falter, because it has information of mainly the most recently passed states and earlier states are hard to remember in this architecture design.\n",
    "\n",
    "\n",
    "This is what give rise to the need of a new architecture that  can solve this problem that the RNN encoder-decoder has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capturing data dependencies with attention mechanism\n",
    "\n",
    "\n",
    "So sometime in 2014, some dude started working on this attention this on the RNN decoder part. Through his work, we could access the input sequence selectively depending on the importance attached to each input in the sequence. Then a question arises. how do we know the importance of each input in the sequence? this is done by the self-attention mechanism which we will see in a bit\n",
    "\n",
    "\n",
    "Using an attention mechanism, the text-generating decoder part can access all the input tokens selectively. this means that some input tokens are more important than others for generating a given output token. The importance is determined by an attention weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attending to different parts of the inputs with self-attention\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
