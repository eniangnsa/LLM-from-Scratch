{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the attention Mechanism\n",
    "\n",
    "We will implement 4 attention mechanism\n",
    "- Simplified self-attention\n",
    "- Self-attention\n",
    "- Causal attention\n",
    "- Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  why do we need  the attention mechanism?\n",
    "\n",
    "Before diving into the details of how the attention mechanism works, it is better to first understand the S-O-T-A models then, which was the RNN encoder-decoder network. This network had its own advantages and was very useful for language translation. It works based on the  principle of using the previous state value and the current input, to compute the next state. This works well for short sentence and texts. But in cases of long sentence, it seems to falter, because it has information of mainly the most recently passed states and earlier states are hard to remember in this architecture design.\n",
    "\n",
    "\n",
    "This is what give rise to the need of a new architecture that  can solve this problem that the RNN encoder-decoder has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capturing data dependencies with attention mechanism\n",
    "\n",
    "\n",
    "So sometime in 2014, some dude started working on this attention this on the RNN decoder part. Through his work, we could access the input sequence selectively depending on the importance attached to each input in the sequence. Then a question arises. how do we know the importance of each input in the sequence? this is done by the self-attention mechanism which we will see in a bit\n",
    "\n",
    "\n",
    "Using an attention mechanism, the text-generating decoder part can access all the input tokens selectively. this means that some input tokens are more important than others for generating a given output token. The importance is determined by an attention weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attending to different parts of the inputs with self-attention\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs  = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # your\n",
    "        [0.55, 0.87, 0.66], # journey\n",
    "        [0.57, 0.85, 0.64], # starts\n",
    "        [0.22, 0.58, 0.33], # with \n",
    "        [0.77, 0.25, 0.10], # one\n",
    "        [0.05, 0.80, 0.55]  # step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is the attention mechanism calculated?\n",
    "\n",
    "first we take the econding for each word and compute the importance of each word with other words in the sequence. This is done by computing the dot product of the input embedding with its transpose. this will yield a new matrix, with the attention scores. this matrix is then normalize for better representation. this will then yield the attention weights which will again be multiplied by the input to get the context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attention_score_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_score_2[i] = torch.dot(x_i, query)\n",
    "print(attention_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# attention scores normalization will lead to the attention weights\n",
    "attention_weight_2 =  attention_score_2 / attention_score_2.sum()\n",
    "print(\"Attention Weights: \", attention_weight_2)\n",
    "print(\"Sum: \", attention_weight_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is advisable to use the softmax normalization as it is more stable and works well if we have negative values in the matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sofmax normalization:   tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "attn_weight_2 =  torch.softmax(attention_score_2, dim=0)\n",
    "print(\"sofmax normalization:  \", attn_weight_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# now let's compute the attention scores for all inputs\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# now normalize the attention scores obtained\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1) # dim=-1 so it normalizes across the rows instead of columns\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context vector for all input sequence:   tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# compute the context vector for each sequence\n",
    "all_context_vec = attn_weights @ inputs\n",
    "print(\"The context vector for all input sequence:  \", all_context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
