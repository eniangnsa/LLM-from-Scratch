{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x103f27650>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "\"the-verdict.txt\")\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with  open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character: \", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'os', ' ', 'a', ' ', 'test.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, os a test.\"\n",
    "result = re.split(r\"(\\s)\", text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'world',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'This',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'os',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'test',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's modify the tokenization\n",
    "result = re.split(r\"([,.]|\\s)\", text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'This', ',', 'os', 'a', 'test', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to strip the whitespace\n",
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'world.', 'Is', 'this', ' -- ', 'a', 'test?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, world. Is this -- a test?\"\n",
    "result =  re.split(r\"([,. :;?_!'()\\\"]-- |\\s)\", text)\n",
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is : 3646\n"
     ]
    }
   ],
   "source": [
    "# let's apply this tokenizer to our raw text\n",
    "preprocessed = re.split(r\"([,. :;?_!'()\\\"]--|\\s)\", raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(f\"The length of the vocabulary is : {len(preprocessed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius--though', 'a', 'good', 'fellow', 'enough--so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that,', 'in', 'the', 'height', 'of', 'his', 'glory,']\n"
     ]
    }
   ],
   "source": [
    "# first  30 tokens\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create  a token id for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is: 1486\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary for the preprocessed text\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"The length of the vocabulary is: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!--', 0)\n",
      "('\"--', 1)\n",
      "('\"Ah,', 2)\n",
      "('\"Ah--I', 3)\n",
      "('\"Be', 4)\n",
      "('\"By', 5)\n",
      "('\"Come', 6)\n",
      "('\"Destroyed', 7)\n",
      "('\"Don\\'t', 8)\n",
      "('\"Gisburns\"', 9)\n",
      "('\"Grindles.\"', 10)\n",
      "('\"Hang', 11)\n",
      "('\"Has', 12)\n",
      "('\"How', 13)\n",
      "('\"I', 14)\n",
      "('\"I\\'d', 15)\n",
      "('\"If', 16)\n",
      "('\"It', 17)\n",
      "('\"It\\'s', 18)\n",
      "('\"Jack', 19)\n",
      "('\"Money\\'s', 20)\n"
     ]
    }
   ],
   "source": [
    "#  now  create the  vocabulary to have each word mapped to a unique id\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "# print a few output of tokens \n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i  >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # preprocessed  = re.split(r'([,.?_!\"()\\']|--\\s)', text)\n",
    "        preprocessed = re.split(r\"([,. :;?_!'()\\\"]--|\\s)\", text)\n",
    "        preprocessed = [\n",
    "            item.strip()  for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \"  \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return  text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1490, 1500, 1494, 1492, 1495, 1502, 1493, 1491, 1489, 1498, 1499, 1501, 1496, 1497]\n"
     ]
    }
   ],
   "source": [
    "new_text  =  \"\"\"\n",
    "    It's the last he painted, you know, Mrs. Gisborn said that with pardonable pride.\n",
    "    \"\"\"\n",
    "preprocessed_2 = re.split(r\"([,. :;?_!'()\\\"]--|\\s)\", new_text)\n",
    "# sort it and add it to all words\n",
    "new_vocab =  sorted(set(preprocessed_2))\n",
    "all_words = all_words + new_vocab\n",
    "# assign new ids to new words in the vocab\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "\n",
    "tokenizer =  SimpleTokenizerV1(vocab)\n",
    "\n",
    "# convert the  text to tokens and ids\n",
    "ids = tokenizer.encode(new_text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's  the  last  he  painted,  you  know,  Mrs.  Gisborn  said  that  with  pardonable  pride.\n"
     ]
    }
   ],
   "source": [
    "# Let's try decoding  the ids\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we had an issue initially when trying to pass a new text that contains unknown words.\n",
    "\n",
    "We ran into the issue unknown words. \n",
    "\n",
    "Next we're going to deal with this issue by extending our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488\n"
     ]
    }
   ],
   "source": [
    "# extend the vocabulary to contain new tokens\n",
    "all_tokens  = sorted(set(preprocessed))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1483)\n",
      "('your', 1484)\n",
      "('yourself', 1485)\n",
      "('<|endoftext|>', 1486)\n",
      "('<|unk|>', 1487)\n"
     ]
    }
   ],
   "source": [
    "# check the last five entries in our vocab\n",
    "for idx, toks in enumerate(list(vocab.items())[-5:]):\n",
    "    print(toks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's update our tokenizer class\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r\"([,.:;?_!'()\\\"]|--|\\s)\", text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r\"\\s+([,.:;?!'()\\\"])\", r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1 = \"Hello, do you like tea?\"\n",
    "text_2 = \"In the sunlit terraces of the palace\"\n",
    "text = \" <|endoftext|> \".join((text_1, text_2))\n",
    "text\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1487, 1487, 489, 1478, 843, 1277, 1487, 1486, 117, 1292, 1257, 1286, 965, 1292, 1487]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|> <|unk|> do you like tea <|unk|> <|endoftext|> In the sunlit terraces of the <|unk|>\n"
     ]
    }
   ],
   "source": [
    "# let's see if we can get the detokenize form of the text in the decoded form\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version: \", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 287, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271]\n"
     ]
    }
   ],
   "source": [
    "# now use the new tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text  = (\n",
    "    \"Hello, do you like tea? <|endoftext|> in the sunlit terraces of someunknownPlace\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> in the sunlit terraces of someunknownPlace\n"
     ]
    }
   ],
   "source": [
    "# let's try to decode the text from the integers\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2348, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "# bpe for uknown token\n",
    "word = \"Alwirw ier\"\n",
    "unk_ids = tokenizer.encode(word)\n",
    "print(unk_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alwirw ier\n"
     ]
    }
   ],
   "source": [
    "# decode the token from the ids\n",
    "unk_decoded =  tokenizer.decode(unk_ids)\n",
    "print(unk_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one major advantage of BPE, it can encode and decode unknown words well without losing the meaning.  It does this with a specialized algortithm, which at the moment i don't know how it works.\n",
    "It works by breaking down words which are not in it's vocabulary into subwords. This is a big plus because it simply does not replace unknown words with the <|unk|> which would cause information loss at the decoding stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [40, 367, 2885, 1464]\n",
      "y: [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "# let's show how the next word prediction works\n",
    "# first we will take a sample of text from our text\n",
    "enc_sample = enc_text[:50]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] ----> 367\n",
      "[40, 367] ----> 2885\n",
      "[40, 367, 2885] ----> 1464\n",
      "[40, 367, 2885, 1464] ----> 1807\n"
     ]
    }
   ],
   "source": [
    "# By processing the inputs along with the targets, we can create next-word prediction task\n",
    "for  i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)\n",
    "    \n",
    "# everything to the left of the arrow shows the input that needs to be fed.\n",
    "# the ones on the right show the target, which is the predicted word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ----->  H\n",
      "I H -----> AD\n",
      "I HAD ----->  always\n",
      "I HAD always ----->  thought\n"
     ]
    }
   ],
   "source": [
    "# let's do it with the decoded text\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    target  = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"----->\", tokenizer.decode([target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data  import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "            return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4,\n",
    "                         max_length=256, stride=128, shuffle=True,\n",
    "                         drop_last=True, num_workers=0):\n",
    "    tokenizer =  tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset  = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1,\n",
    "    max_length=4, \n",
    "    stride=1, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch =  next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first tensor above is the input ids for the text, the second tensor is the target token ids. Since the max_length=4, each tensor contains 4 token ids\n",
    "\n",
    "- The stride shows how many context windows should in sampling the next input text for each batch . \n",
    "- When creating multiple batches from the input dataset, we slide an input window across the text. If the stride is set to 1, we shift the input window by one position when creating the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Dataloaders with different strides and context length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "targets: \n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# use a stride of 4 and max length of 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8,\n",
    "    max_length=4, stride=4, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter) \n",
    "print(\"Inputs: \\n\", inputs)\n",
    "print(\"targets: \\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Token Embeddings\n",
    "\n",
    "So far what we have done in processing our raw text is to tokenize it and assign ids to each token. The BPE tokenizer does this for us well. But before we feed the input into the llm model to work with, we still have another step to do. The token embedding. Here we convert the token ids into embedding vectors.\n",
    "\n",
    "\n",
    "As a preliminary step, we must initialize these embedding weights with random values. This initialization serves as the starting point for the LLM's learning process. The weights of this embeddings will be optimized during the training process.\n",
    "\n",
    "So far we have covered the following:\n",
    "- tokenization of texts\n",
    "- convert tokens to token ids\n",
    "- convert token ids to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])\n",
    "vocab_size = 6\n",
    "output_dim =  3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# apply it to the input token ids\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding word position\n",
    "We have two broad approaches here:\n",
    "- relative positional embeddings\n",
    "- absolute positional embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      " inputs shape: \n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "# Let's now be more realistic with our embeddings\n",
    "vocab_size = 50257\n",
    "output_dim  =  256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "max_length =  4\n",
    "dataloader  =  create_dataloader_v1(\n",
    "    raw_text,  batch_size=8, max_length=4, stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets =  next(data_iter)\n",
    "print(\"Token id: \\n\", inputs)\n",
    "print(\"\\n inputs shape: \\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# use the embedding layers to embedd the token ids into 256-dimensional vectors\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT model embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embedding =  token_embeddings  +  pos_embedding\n",
    "print(input_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
