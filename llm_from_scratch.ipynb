{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x103f27650>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "\"the-verdict.txt\")\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with  open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character: \", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'os', ' ', 'a', ' ', 'test.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, os a test.\"\n",
    "result = re.split(r\"(\\s)\", text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'world',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'This',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'os',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'test',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's modify the tokenization\n",
    "result = re.split(r\"([,.]|\\s)\", text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'This', ',', 'os', 'a', 'test', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to strip the whitespace\n",
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'world.', 'Is', 'this', ' -- ', 'a', 'test?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, world. Is this -- a test?\"\n",
    "result =  re.split(r\"([,. :;?_!'()\\\"]-- |\\s)\", text)\n",
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is : 3646\n"
     ]
    }
   ],
   "source": [
    "# let's apply this tokenizer to our raw text\n",
    "preprocessed = re.split(r\"([,. :;?_!'()\\\"]--|\\s)\", raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(f\"The length of the vocabulary is : {len(preprocessed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius--though', 'a', 'good', 'fellow', 'enough--so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that,', 'in', 'the', 'height', 'of', 'his', 'glory,']\n"
     ]
    }
   ],
   "source": [
    "# first  30 tokens\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create  a token id for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is: 1486\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary for the preprocessed text\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"The length of the vocabulary is: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!--', 0)\n",
      "('\"--', 1)\n",
      "('\"Ah,', 2)\n",
      "('\"Ah--I', 3)\n",
      "('\"Be', 4)\n",
      "('\"By', 5)\n",
      "('\"Come', 6)\n",
      "('\"Destroyed', 7)\n",
      "('\"Don\\'t', 8)\n",
      "('\"Gisburns\"', 9)\n",
      "('\"Grindles.\"', 10)\n",
      "('\"Hang', 11)\n",
      "('\"Has', 12)\n",
      "('\"How', 13)\n",
      "('\"I', 14)\n",
      "('\"I\\'d', 15)\n",
      "('\"If', 16)\n",
      "('\"It', 17)\n",
      "('\"It\\'s', 18)\n",
      "('\"Jack', 19)\n",
      "('\"Money\\'s', 20)\n"
     ]
    }
   ],
   "source": [
    "#  now  create the  vocabulary to have each word mapped to a unique id\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "# print a few output of tokens \n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i  >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # preprocessed  = re.split(r'([,.?_!\"()\\']|--\\s)', text)\n",
    "        preprocessed = re.split(r\"([,. :;?_!'()\\\"]--|\\s)\", text)\n",
    "        preprocessed = [\n",
    "            item.strip()  for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \"  \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return  text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1490, 1500, 1494, 1492, 1495, 1502, 1493, 1491, 1489, 1498, 1499, 1501, 1496, 1497]\n"
     ]
    }
   ],
   "source": [
    "new_text  =  \"\"\"\n",
    "    It's the last he painted, you know, Mrs. Gisborn said that with pardonable pride.\n",
    "    \"\"\"\n",
    "preprocessed_2 = re.split(r\"([,. :;?_!'()\\\"]--|\\s)\", new_text)\n",
    "# sort it and add it to all words\n",
    "new_vocab =  sorted(set(preprocessed_2))\n",
    "all_words = all_words + new_vocab\n",
    "# assign new ids to new words in the vocab\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "\n",
    "tokenizer =  SimpleTokenizerV1(vocab)\n",
    "\n",
    "# convert the  text to tokens and ids\n",
    "ids = tokenizer.encode(new_text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's  the  last  he  painted,  you  know,  Mrs.  Gisborn  said  that  with  pardonable  pride.\n"
     ]
    }
   ],
   "source": [
    "# Let's try decoding  the ids\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we had an issue initially when trying to pass a new text that contains unknown words.\n",
    "\n",
    "We ran into the issue unknown words. \n",
    "\n",
    "Next we're going to deal with this issue by extending our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488\n"
     ]
    }
   ],
   "source": [
    "# extend the vocabulary to contain new tokens\n",
    "all_tokens  = sorted(set(preprocessed))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1483)\n",
      "('your', 1484)\n",
      "('yourself', 1485)\n",
      "('<|endoftext|>', 1486)\n",
      "('<|unk|>', 1487)\n"
     ]
    }
   ],
   "source": [
    "# check the last five entries in our vocab\n",
    "for idx, toks in enumerate(list(vocab.items())[-5:]):\n",
    "    print(toks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's update our tokenizer class\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r\"([,.:;?_!'()\\\"]|--|\\s)\", text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r\"\\s+([,.:;?!'()\\\"])\", r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1 = \"Hello, do you like tea?\"\n",
    "text_2 = \"In the sunlit terraces of the palace\"\n",
    "text = \" <|endoftext|> \".join((text_1, text_2))\n",
    "text\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1487, 1487, 489, 1478, 843, 1277, 1487, 1486, 117, 1292, 1257, 1286, 965, 1292, 1487]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|> <|unk|> do you like tea <|unk|> <|endoftext|> In the sunlit terraces of the <|unk|>\n"
     ]
    }
   ],
   "source": [
    "# let's see if we can get the detokenize form of the text in the decoded form\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version: \", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 287, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271]\n"
     ]
    }
   ],
   "source": [
    "# now use the new tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text  = (\n",
    "    \"Hello, do you like tea? <|endoftext|> in the sunlit terraces of someunknownPlace\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> in the sunlit terraces of someunknownPlace\n"
     ]
    }
   ],
   "source": [
    "# let's try to decode the text from the integers\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
